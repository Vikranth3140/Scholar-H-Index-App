<html><body><h2>Results for chunk 1/3</h2><h3>Result 1:</h3><p>Title: Sus-x: Training-free name-only transfer of vision-language models</p><p>URL: http://openaccess.thecvf.com/content/ICCV2023/html/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.html</p><p>Snippet: Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot …</p><br><h3>Result 2:</h3><p>Title: Cobra: Contrastive bi-modal representation algorithm</p><p>URL: https://arxiv.org/abs/2005.03687</p><p>Snippet: There are a wide range of applications that involve multi-modal data, such as cross-modal retrieval, visual question-answering, and image captioning. Such applications are primarily …</p><br><h3>Result 3:</h3><p>Title: EDUQA: Educational domain question answering system using conceptual network mapping</p><p>URL: https://ieeexplore.ieee.org/abstract/document/8683538/</p><p>Snippet: Most of the existing question answering models can be largely compiled into two categories: i) open domain question answering models that answer generic questions and use large-…</p><br><h3>Result 4:</h3><p>Title: Memeify: A large-scale meme generation system</p><p>URL: https://dl.acm.org/doi/abs/10.1145/3371158.3371403</p><p>Snippet: Interest in the research areas related to meme propagation and generation has been increasing rapidly in the last couple of years. Meme datasets available online are either specific to …</p><br><h3>Result 5:</h3><p>Title: Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models</p><p>URL: https://arxiv.org/abs/2310.08577</p><p>Snippet: Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of …</p><br><h3>Result 6:</h3><p>Title: On the inference of soft biometrics from typing patterns collected in a multi-device environment</p><p>URL: https://ieeexplore.ieee.org/abstract/document/9232527/</p><p>Snippet: In this paper, we study the inference of gender, major/minor (computer science, non-computer science), typing style, age, and height from the typing patterns collected from 117 …</p><br><h3>Result 7:</h3><p>Title: Changing landscape of technical education pedagogy from traditional to practical e-learning</p><p>URL: https://ieeexplore.ieee.org/document/9928242/</p><p>Snippet: With the COVID-19 pandemic, online university education assumed greater importance. We propose a practical e-learning model with suggestions to augment online education of all …</p><br><h3>Result 8:</h3><p>Title: Discont: Self-supervised visual attribute disentanglement using context vectors</p><p>URL: https://link.springer.com/chapter/10.1007/978-3-030-65414-6_38</p><p>Snippet: Disentangling underlying feature attributes within an image with no prior supervision is a challenging task. Models that can disentangle attributes well, provide greater interpretability …</p><br><h3>Result 9:</h3><p>Title: InPHYNet: Leveraging attention-based multitask recurrent networks for multi-label physics text classification</p><p>URL: https://www.sciencedirect.com/science/article/pii/S095070512030616X</p><p>Snippet: The ability to create and sustain educational infrastructure is a major challenge to nations across the world. Today, information technology is increasingly being used to alleviate this …</p><br><h3>Result 10:</h3><p>Title: Understanding and Fixing the Modality Gap in Vision-Language Models</p><p>URL: https://www.mlmi.eng.cam.ac.uk/files/2021-2022_dissertations/understanding_and_fixing_the_modality_gap_in_vision-language_models_reduced.pdf</p><p>Snippet: Contrastive language-image pre-training has emerged to be a simple yet effective way to train largescale vision-language models [165, 83, 181, 220] that are capable of learning …</p><br><h2>Results for chunk 2/3</h2><h3>Result 1:</h3><p>Title: It's LeVAsa not LevioSA! Latent Encodings for Valence-Arousal Structure Alignment</p><p>URL: https://dl.acm.org/doi/abs/10.1145/3430984.3431037</p><p>Snippet: In recent years, great strides have been made in the field of affective computing. Several models have been developed to represent and quantify emotions. Two popular ones include (i) …</p><br><h3>Result 2:</h3><p>Title: COBRA: Contrastive Bi-Modal Representation Learning</p><p>URL: http://cobweb.cs.uga.edu/~shengli/Docs/Tusion20/4_COBRA_Contrastive_Bi-Modal_Representation_Learning.pdf</p><p>Snippet: There are a wide range of applications that involve multi-modal data, such as cross-modal retrieval, visual question-answering and image captioning. Such applications are primarily …</p><br><h3>Result 3:</h3><p>Title: Memeify: A Large-Scale Meme Generation System</p><p>URL: https://ui.adsabs.harvard.edu/abs/2019arXiv191012279R/abstract</p><p>Snippet: Interest in the research areas related to meme propagation and generation has been increasing rapidly in the last couple of years. Meme datasets available online are either specific to …</p><br><h3>Result 4:</h3><p>Title: Self Learning Chat Bot to answer frequently asked questions from teachers</p>